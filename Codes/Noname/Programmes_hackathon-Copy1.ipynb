{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques programmes utilisés lors du Hackathon\n",
    "\n",
    "Voici une partie des programmes utilisés lors du Hackathon. Le groupe disposait d'un programme de matching plus élaboré. Ce programme était capable d'identifier certaines structures des chaines de caractères à partir de chaines de markov cachées. Le travail présenté était destiné à enrichir cette approche. Il n'est pas destiné à fonctionner de façon autonome. Ceci explique ses piètres performances. \n",
    "\n",
    "Trois directions ont été explorés :\n",
    "1. Le webscraping des pages jaunes afin de trouver certaines adresses\n",
    "2. Le codage de l'activité (code NAF sur 2 positions) à partir de l'activité déclarée au recensement.\n",
    "3. L'identification des groupes de raisons sociales les plus problématiques (éducation, défence etc.)\n",
    "\n",
    "Les points 2 et 3 sont sans doute les plus intéressants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prérequis\n",
    "\n",
    "### Importation des librairies requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importation des librairies requises\n",
    "\n",
    "import re\n",
    "import string as txt\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer #import the French stemming library\n",
    "from nltk.corpus import stopwords #import stopwords from nltk corpus\n",
    "import operator\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques fonctions de manipulation de chaine de caractères\n",
    "\n",
    "Une première fonction de nettoyage de données textuelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    string = str(string)\n",
    "    # suppression des accents\n",
    "    string = unidecode.unidecode(string)\n",
    "    # Remplace les signes de ponctuations\n",
    "    for c in txt.punctuation :\n",
    "        string = re.sub(re.escape(c),' ',string)\n",
    "    string = string.strip()\n",
    "    # remplacement des sigles\n",
    "    s = re.findall(r'\\s(\\w\\s\\w)\\s',string)\n",
    "    if len(s)>0 : string = re.sub(' '+s[0]+' ',re.sub(' ','',s[0]),string)\n",
    "    # Mise en majuscules\n",
    "    string = string.upper()\n",
    "    # Suppression des espaces inutiles\n",
    "    string = string.strip()  \n",
    "    # Remplace les tabluations, sauts de lignes, etc. par des espaces.\n",
    "    string = re.sub('\\n|\\r|\\t|\\xa0', ' ', string)\n",
    "    # Retire les ' .' (séparateurs utilisés dans `soup.get_text`) inappropriés.\n",
    "    string = re.sub('^\\.', '', string.replace(' .', ''))\n",
    "    # Retire les espaces inappropriés.\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fonction de comptage du nombre de mots dans une colonne (vecteur ?) au format texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comptage_mots(colonne):\n",
    "    \n",
    "    # concatenation de la colonne \n",
    "    words = \" \".join(colonne)\n",
    "    # Nettoyage sommaire des donnees textuelles\n",
    "    words = re.sub('\\n','',words)\n",
    "    words = re.sub('[^A-Z\\s]|','',words)\n",
    "    words = words.strip()\n",
    "    tokens = nltk.word_tokenize(words,language='french')\n",
    "\n",
    "    # Nettoyage de la chaine de caracteres des stopwords\n",
    "    clean_tokens = tokens[:]\n",
    "    for token in tokens:\n",
    "        if token.lower() in stopwords.words('french'):\n",
    "            clean_tokens.remove(token)\n",
    "    # Affichage pour verification\n",
    "    print(clean_tokens[:100])    \n",
    "\n",
    "    # Comptage du nombre de mots\n",
    "    fidist2 = nltk.FreqDist(clean_tokens)\n",
    "\n",
    "    # tri des mots les plus frequents\n",
    "    fidist2 = sorted(fidist2.items(), reverse=True, key=operator.itemgetter(1))\n",
    "    \n",
    "    return fidist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeux de données chargé ; 30876 lignes, 52 colonnes.\n"
     ]
    }
   ],
   "source": [
    "# gestion des tables de donnees\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "\n",
    "# Importation des donnees du RP\n",
    "rp = pd.read_csv(\"/Users/stephaniehimpens/Documents/Hackathon/donnees/Donnees/rp_final_2014.csv\",sep=\";\",dtype=str)\n",
    "print(\"Jeux de données chargé ; \" + str(rp.shape[0]) + \" lignes, \" + str(rp.shape[1]) + \" colonnes.\")\n",
    "\n",
    "# Recodage des donnees manquantes\n",
    "rp.loc[rp.NUMVOI_X.isnull(),\"NUMVOI_X\"]=\"\"\n",
    "rp.loc[rp.BISTER_X.isnull(),\"BISTER_X\"]=\"\"\n",
    "rp.loc[rp.TYPEVOI_X.isnull(),\"TYPEVOI_X\"]=\"\"\n",
    "rp.loc[rp.NOMVOI_X.isnull(),\"NOMVOI_X\"]=\"\"\n",
    "rp.loc[rp.CPLADR_X.isnull(),\"CPLADR_X\"]=\"\"\n",
    "\n",
    "rp.NUMVOI_X = [str(int(\"0\" + word)) for word in rp.NUMVOI_X]\n",
    "rp.loc[rp.NUMVOI_X=='0',\"NUMVOI_X\"]=''\n",
    "\n",
    "\n",
    "# Concatenation des champs de l adresse\n",
    "rp.ADR_FULL = rp.NUMVOI_X + \" \" + rp.BISTER_X + \" \" + rp.TYPEVOI_X + \" \"+ rp.NOMVOI_X + \" \" + rp.CPLADR_X\n",
    "sup_blanc = lambda x:x.strip()\n",
    "rp.ADR_FULL = rp.ADR_FULL.map(sup_blanc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de minimiser le temps de traitement, l'appariement a été réalisé sur un échantillon de 500 observations du RP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniehimpens/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# tirage aleatoire\n",
    "import random as rd\n",
    "\n",
    "#tirage aleatoire des donnees du rp\n",
    "id_ech = numpy.random.choice(range(rp.shape[0]),size=500,replace=False)\n",
    "rp = rp.ix[id_ech,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nettoyage des RS du recensement  \n",
    "# remplacement des codes communes\n",
    "for index,row in rp.iterrows():\n",
    "    depcom = str(row.DEPCOM_CODE)\n",
    "    depcom = str(row.CLT_X) + ' ' + depcom[0:2]\n",
    "    row.RS_X = re.sub(depcom,'',str(row.RS_X))   \n",
    "    row.RS_X = re.sub(str(row.CLT_X),'',str(row.RS_X))\n",
    "    tags = tagger.tag_text(row.RS_X)\n",
    "    c=''\n",
    "    for t in range(len(tags)) :\n",
    "       if t=='' : c = c + \" \" + str(tags[t]).split('\\t')[2]\n",
    "       c = c.strip()\n",
    "    if len(c)>0 : row.RS_X = c\n",
    "    rp.ix[index,\"RS_X\"] = row.RS_X\n",
    "rp.RS_X = [clean_text(word) for word in rp.RS_X]\n",
    "rp.CLT_X = [clean_text(word) for word in rp.CLT_X]\n",
    "rp.CLT_X = [re.sub('ST ','SAINT ',word) for word in rp.CLT_X]\n",
    "rp.CLT_X = [re.sub(r'[0-9]','',word).strip() for word in rp.CLT_X]\n",
    "rp.CLT_X = [re.sub(r'[0-9]','',word).strip() for word in rp.CLT_X]\n",
    "rp.CLT_X = [re.sub('NAN','',word).strip() for word in rp.CLT_X]\n",
    "\n",
    "# Remplacements specifiques\n",
    "rp.RS_X = [re.sub('VILLE','COMMUNE'+str(rp.CLT_X),word) for word in rp.RS_X]\n",
    "rp.RS_X = [re.sub('MAIRIE','COMMUNE'+str(rp.CLT_X),word) for word in rp.RS_X]\n",
    "rp.RS_X = [re.sub('LBV YVES ROCHER','YVES ROCHER',word) for word in rp.RS_X]\n",
    "rp.RS_X = [re.sub('CHU ','HOPITAL',word) for word in rp.RS_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le webscraping des pages jaunes à partir de la raison sociale et de la commune\n",
    "\n",
    "Une fonction de webscraping a été écrite mais elle n'a finalement pas été utilisée. Les pages jaunes ont été bloqués pendant une bonne partie du Hackathon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installation des librairies tierces utilisées.\n",
    "import pip\n",
    "for pkg in ['requests', 'bs4']:\n",
    "    pip.main(['install', pkg])\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection des observations du recensement sans adresse. On n'en retient que 6. Il s'agit de tester le programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testsorted = []\n",
    "selection = rp.loc[rp.ADR_FULL=='']\n",
    "selection = selection[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le programme boucle sur la table (récupération du nom de la commune (ou du numéro de département si non disponible) et de la raison sociale) et construit les requêtes pour le site des pages jaunes.\n",
    "Les résultats sont récupérés dans une table panda (rp \"enrichi\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,row in selection.iterrows():\n",
    "    # Mise en forme de la requete\n",
    "    if row.CLT_X=='':\n",
    "        if str(row.DLT_X)=='nan':\n",
    "            test = 'https://www.pagesjaunes.fr/recherche/'+str(row.DEPCOM_CODE[:2])+'/'+row.RS_X\n",
    "        else :\n",
    "            test = 'https://www.pagesjaunes.fr/recherche/'+str(row.DLT_X)+'/'+row.RS_X\n",
    "    else :\n",
    "        test = 'https://www.pagesjaunes.fr/recherche/'+str(row.CLT_X)+'/'+str(row.RS_X)\n",
    "    test = str(test)\n",
    "    test = re.sub(\"\\[\",\"\",test)\n",
    "    test = re.sub(\"\\]\",\"\",test)\n",
    "    test = re.sub(\"'\",\"\",test)\n",
    "    test = re.sub('\"','',test)\n",
    "    \n",
    "    # Interrogation des pages jaunes\n",
    "    response = requests.get(test)\n",
    "    text = re.sub('denomination-links pj-link','denomination-links pj-lb pj-link',response.text)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        print('Échec de la requête: statut %s.' % response.status_code)\n",
    "\n",
    "    # Mise en forme des donnees textuelles\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    rsp=[]\n",
    "    adrp=[]\n",
    "    actp=[]\n",
    "    supp=[]\n",
    "    for p in soup.find_all('article'):\n",
    "        a = p.find_all('a',{'class','denomination-links pj-lb pj-link'})\n",
    "        if len(a)==0:\n",
    "            a=BeautifulSoup(\"<a> </a>\", 'html.parser')\n",
    "        rsp.extend(a)\n",
    "        a = p.find_all('a',{'class','adresse pj-lb pj-link'})\n",
    "        if len(a)==0:\n",
    "            a=BeautifulSoup(\"<a> </a>\", 'html.parser')\n",
    "        adrp.extend(a)\n",
    "        a = p.find_all('a',{'class','activites pj-lb pj-link'})\n",
    "        if len(a)==0:\n",
    "            a=BeautifulSoup(\"<a> </a>\", 'html.parser')\n",
    "        actp.extend(a)\n",
    "        a = p.find_all('span',{'class','denoms-suppl'})\n",
    "        if len(a)==0:\n",
    "            a=BeautifulSoup(\"<a> </a>\", 'html.parser')\n",
    "        supp.extend(a)\n",
    "    \n",
    "    if len(rsp)>0:\n",
    "        rs = [clean_text(r.text) for r in rsp]\n",
    "        adresse = [clean_text(r.text) for r in adrp]\n",
    "        activite = [clean_text(r.text) for r in actp]\n",
    "        if supp is not None:\n",
    "            s = [clean_text(su.text) for su in supp]\n",
    "            s = [re.sub('CONNU SOUS ','',su) for su in s]\n",
    "        else:    \n",
    "            s = [\"\"]\n",
    "    else:\n",
    "        rs = [\"\"]\n",
    "        adresse = [\"\"]\n",
    "        activite = [\"\"]\n",
    "        s=[\"\"]\n",
    "        \n",
    "    test2 = [(row.CABBI,rs[i],adresse[i],activite[i],s[i]) for i in range(min(len(rs),3))]\n",
    "    print(test2)\n",
    "    if len(test2)==0 :\n",
    "        test2 = [(row.CABBI,'','','','')]\n",
    "    # tri des mots les plus frequents\n",
    "    if len(testsorted)==0:\n",
    "        testsorted = test2\n",
    "    else :\n",
    "        testsorted.extend(test2)\n",
    "    \n",
    "    # arret 5 minutes afin de simuler le comportement d un humain\n",
    "    time.sleep(5)\n",
    "    \n",
    "\n",
    "labels = ['CABBI', 'RS_PJ','ADR_PJ', 'ACT_PJ', 'SP_PJ']\n",
    "df = pd.DataFrame.from_records(testsorted, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La nouvelle table est exportée au format csv. Les données n'ont finalement pas servi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rp.to_csv(\"/Users/stephaniehimpens/Documents/Hackathon/donnees/data_IlleEtVilaine/rp_enrichi.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le codage de l'activité (code NAF sur 2 positions) à partir de l'activité déclarée au RP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des librairies prérequises :\n",
    "(TreeTagger doit faire l'objet d'une installation préalable sur le poste de travail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pprint   # For proper print of sequences.\n",
    "import treetaggerwrapper\n",
    "#1) build a TreeTagger wrapper:\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr',TAGDIR='/Users/stephaniehimpens/Documents/teetagger')\n",
    " #2) tag your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du dictionnaire en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "# Correction orthographique\n",
    "\n",
    "d = enchant.Dict(\"fr_FR\")\n",
    "# dictionnaire personnel\n",
    "#d = enchant.request_pwl_dict(\"/Users/stephaniehimpens/Documents/Hackathon/donnees/dico.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en forme des données du RP sous forme d'une liste de mots nettoyée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a052614218c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import des donnees du RP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdon_rp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/stephaniehimpens/Documents/Hackathon/donnees/Donnees/rp_final_2014.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Latin1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Nettoyage des RS du recensement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Import des donnees du RP\n",
    "don_rp = pd.read_csv('/Users/stephaniehimpens/Documents/Hackathon/donnees/Donnees/rp_final_2014.csv',delimiter=';',encoding='Latin1',low_memory=False)\n",
    "\n",
    "\n",
    "# Nettoyage des RS du recensement  \n",
    "don_rp.ACTET_X = [clean_text(word) for word in don_rp.ACTET_X]\n",
    "\n",
    "# Mise en forme des mots\n",
    "ACTET_X = [re.sub('[^A-Z\\s]|','',str(words)) for words in don_rp.ACTET_X]\n",
    "ACTET_X = [words.strip() for words in ACTET_X]\n",
    "# Jetons en langue francaise\n",
    "tokens = [(nltk.word_tokenize(words,language='french')) for words in ACTET_X]\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    clean = []\n",
    "    for c in token :\n",
    "        if c.lower() in stopwords.words('french') or c=='A' or c=='NAN':\n",
    "            clean = clean\n",
    "        else:\n",
    "            tags = tagger.tag_text(c)\n",
    "            if len(tags)>0 : c = str(tags[0]).split('\\t')[2]\n",
    "            if d.check(c.upper()) : clean.append(c)\n",
    "            # partie de recherche dans le dictionnaire trop longue => desactivee\n",
    "            #else : \n",
    "             #   sug = d.suggest(c.upper())\n",
    "                #if len(sug)>0 : \n",
    "                 #   clean.append(sug[0]) \n",
    "                #else : clean.append(c)\n",
    "    clean = [word.upper() for word in clean]\n",
    "    #if len(clean)>0 : clean = clean[0]\n",
    "    clean = \" \".join(clean)\n",
    "    clean_tokens.append(clean)\n",
    "print(clean_tokens[:100])    \n",
    "\n",
    "# Injection des mots dans la table\n",
    "test = don_rp\n",
    "test.ACTET_X = [str(word) for word in clean_tokens]\n",
    "test = test.loc[test.ACTET_X.isnull()==False]\n",
    "test = test.loc[test.ACTET_X!='']\n",
    "test = test.loc[test.ACTET_C.isnull()==False]\n",
    "\n",
    "# recuperation du numero de departement\n",
    "categ = [str(ligne)[0:2] for ligne in test.ACTET_C]\n",
    "# selection des donnees\n",
    "test = test.loc[[len(text)>1 for text in categ]]\n",
    "categ = [text for text in categ if len(text)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation de la table en échantillon test et échantillon d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tirage d un echantillon\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(test['ACTET_X'], categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustement d'un modèle SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification SVM\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "              ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                       alpha=1e-3, n_iter=10, random_state=42)),\n",
    " ])\n",
    " \n",
    "\n",
    "_ = text_clf_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur l'échantillon d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "print(np.mean([predicted_svm == y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification des groupes les plus représentés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit dans cette partie de repérer les ensembles de mots revenant souvent. L'idée est qu'il ne faut pas se tromper pour les employeurs appartenant à ces groupes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le travail préliminaire de mise en forme des données textuelles ressemble à celui du paragraphe précédent.\n",
    "Chargement des librairies requises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement et mise en forme des données du RP (découpage en mots, suppression des mots vides, \"taggage\" des mots) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "don_rp = pd.read_csv('/Users/stephaniehimpens/Documents/Hackathon/donnees/Donnees/rp_final_2014.csv',delimiter=';',encoding='Latin1',low_memory=False)\n",
    "\n",
    "documents = [clean_text(word) for word in don_rp.RS_X]\n",
    "\n",
    "\n",
    "doc = [re.sub('[^A-Z\\s]|','',str(words)) for words in documents]\n",
    "doc = [words.strip() for words in doc]\n",
    "tokens = [(nltk.word_tokenize(words,language='french')) for words in doc]\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    clean = []\n",
    "    for c in token :\n",
    "        if c.lower() in stopwords.words('french') or c=='A' or c=='NAN':\n",
    "            clean = clean\n",
    "        else:\n",
    "            tags = tagger.tag_text(c)\n",
    "            if len(tags)>0 : c = str(tags[0]).split('\\t')[2]\n",
    "            clean.append(c)\n",
    "            #if d.check(c.upper()) : clean.append(c)\n",
    "            #else : clean.append(d.suggest(c.upper())[0])\n",
    "    clean = [word.upper() for word in clean]\n",
    "    #if len(clean)>0 : clean = clean[0]\n",
    "    clean = \" \".join(clean)\n",
    "    clean_tokens.append(clean)\n",
    "print(clean_tokens[:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Première tentative de clusteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Premiere version\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X =  vectorizer.fit_transform(cols)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(X)\n",
    "print\n",
    "print\n",
    "\n",
    " \n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième tentative (infructueuse) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Latent semantic Analysis\n",
    "# Nouvel essai\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(clean_tokens)\n",
    "\n",
    "vectorizer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(X_train_counts)\n",
    "\n",
    "svd = TruncatedSVD(141)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "# trop de clusters ici => trouver le nombre optimal ?\n",
    "km = KMeans(n_clusters=141, init='k-means++', max_iter=100)\n",
    "km.fit(X)\n",
    "model=km\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des clusters obtenus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualisation des clusters\n",
    "clusters = model.labels_.tolist()\n",
    " \n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    " \n",
    " \n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un programme sommaire d'appariement des données a également été écrit mais il n'est pas particulièrement intéressant et il n'a pas été reproduit ici. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
