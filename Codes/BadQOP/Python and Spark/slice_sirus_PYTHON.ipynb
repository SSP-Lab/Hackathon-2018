{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code a été développé lors du hackathon Insee 2018 dans l'objectif de lire le fichier SIRUS. Ce fichier étant trop lourd pour tenir en mémoire, il nous a fallu ruser et créer un autre csv, avec uniquement les informations nous intéressant dans SIRUS (les informations concernantles APET des secteurs O, P et Q.\n",
    "\n",
    "Description technique du problème : une colonne de SIRUS désigne les codes APET, et nous n'avions besoin que des lignes dont les APET commençaient par \"84\", \"85\", \"86\", \"87\", \"88\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\" ou \"99\".\n",
    "Résolution : on lit le fichier SIRUS par groupe de 100 000 lignes, on ne conserve de chaque groupe que les lignes ayant les bons APET, que l'on ajoute DataFrame, qui se remplit donc progressivement. A la fin, on dispose d'un DataFrame ayant l'information que l'on souhaite, et on l'exporte en csv.\n",
    "\n",
    "Pistes pour une utilité plus large : en appliquant une autre fonction que \"right_apet\", on peut créer un sous-échantillon de n'importe quel fichier sur n'importe quelle condition sur n'importe quelle colonne. Le script qui suit est une ébauche pour la sélection d'information dans de csv, aussi gros soient-il."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# chemin a modifier\n",
    "os.chdir(\"C://Users//alize//OneDrive//Documents//ENSAE//cesure//hackathon_insee\")\n",
    "filepath = \"./données/DonneesTransmisesHackathon/sirus_2017.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_apet(apet_to_inspect, list_right_apet):\n",
    "    \"\"\"\n",
    "    Return a list of booleans with the index of APET starting with one of the right APET.\n",
    "    \"\"\"\n",
    "    if isinstance(apet_to_inspect, str):\n",
    "        return apet_to_inspect[:2] in list_right_apet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sirus_id;nic;ape;apet;eff_3112_et;eff_etp_et;eff_et_effet_daaaammjj;enseigne_et1;nom_comm_et;adr_et_loc_geo;adr_et_compl;adr_et_voie_num;adr_et_voie_repet;adr_et_voie_type;adr_et_voie_lib;adr_et_cedex;adr_et_distsp;sir_adr_et_com_lib;adr_et_post;adr_et_l1;adr_et_l2;adr_et_l3;adr_et_l4;adr_et_l5;adr_et_l6;adr_et_l7;nic_siege;unite_type;region;adr_depcom;region_impl;region_mult;tr_eff_etp;cj;denom;denom_condense;sigle;enseigne;eff_3112_uniteLegale;eff_etp_uniteLegale;eff_effet_daaaammjj_uniteLegale;x;y;SourceXYW;qual\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Premier regard au csv, pour en connaitre les colonnes\n",
    "# et le delimiteur.\n",
    "with open(filepath, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "    print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Répartition des groupes de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8526231\n"
     ]
    }
   ],
   "source": [
    "# On compte le nombre de lignes (execution un peu longue, tout le fichier doit\n",
    "# etre lu).\n",
    "# NB : il aurait ete possible d'eviter cette etape : lire le fichier k lignes\n",
    "# par k lignes et accepter que le dernier groupe de lignes\n",
    "# puisse avoir moins de k lignes.\n",
    "num_lines = sum(1 for line in open(filepath))\n",
    "all_rows = list(range(num_lines + 1))\n",
    "print(num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header_row = 0\n",
    "end = num_lines + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture par groupe de lignes du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation : on prepare le DataFrame vide que l'on va \n",
    "# progressivement remplir.\n",
    "df_all_header = pd.read_csv(filepath, sep=\";\", header=0, nrows=10, encoding=\"latin-1\")\n",
    "df_to_keep = pd.DataFrame(columns = df_all_header.columns)\n",
    "\n",
    "# On fixe les APET que l'on cherche.\n",
    "list_right_apet = [\"84\", \"85\", \"86\", \"87\", \"88\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\", \"98\", \"99\"]\n",
    "\n",
    "# On fixe le nombre d'elements par groupes (et donc le nombre de groupes).\n",
    "size_group = 100000\n",
    "nb_groups = int(end / size_group)\n",
    "groups = {}\n",
    "\n",
    "for i in range(nb_groups):\n",
    "    # construction des index des lignes a ajouter a df_to_keep\n",
    "    if i != nb_groups:\n",
    "        groups[i] = list(range(size_group * i + 1, size_group * (i+1)))\n",
    "        groups[i].append(header_row)\n",
    "    else:\n",
    "        groups[i] = list(range(size_group * i + 1, num_lines))\n",
    "        groups[i].append(header_row)\n",
    "    # on en deduit les index a ne pas conserver\n",
    "    rows_to_exclude = list(set(all_rows).symmetric_difference(groups[i]))\n",
    "    # on lit le fichier Sirus et on en extrait les lignes correspondant\n",
    "    # aux APET qui nous interessent.\n",
    "    df_all = pd.read_csv(filepath, sep=\";\", header=0, skiprows=rows_to_exclude, encoding=\"latin-1\")\n",
    "    df_all[\"apet\"] = df_all[\"apet\"].apply(lambda x: str(x))\n",
    "    tmp_to_keep = df_all[df_all[\"apet\"].apply(lambda x: right_apet(x, list_right_apet))]\n",
    "    # on ajoute ces lignes a df_to_keep\n",
    "    df_to_keep = df_to_keep.append(tmp_to_keep)\n",
    "    print(\"Finished with group \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation du résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_keep[\"apet\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
